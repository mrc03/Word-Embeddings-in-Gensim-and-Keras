{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTING MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "% matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#stop-words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# tokenizing\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATING SAMPLE CORPUS OF DOCUMENTS ie TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_1=\"one in hand is better than two in bush\"\n",
    "sample_text_2=\"bush was the president of US\"\n",
    "sample_text_3=\"India has only one president\"\n",
    "\n",
    "corp=[sample_text_1,sample_text_2,sample_text_3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTEGER ENCODING ALL THE DOCUMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after this all the unique words will be reprsented by an integer. for this we are using one_hotnfunction from the Keras. note that the vocab_size is specified large enough so as to ensure unique integer encoding for each and every word.\n",
    "\n",
    "**note one important thing that the value of word remains same in different docs. eg 'one' is 14 in both first and third documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding for document 1  is :  [10, 35, 45, 30, 8, 43, 44, 35, 26]\n",
      "The encoding for document 2  is :  [26, 17, 44, 3, 26, 19]\n",
      "The encoding for document 3  is :  [36, 33, 22, 10, 3]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=50  # take large enough to avoid chances of different words having same integr index. or can write your own function.\n",
    "encod_corp=[]\n",
    "for i,doc in enumerate(corp):\n",
    "    encod_corp.append(one_hot(doc,50))\n",
    "    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PADDING THE DOCS (to make very doc of same length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Keras Embedding layer requires all individual documents to be of same length. hence we wil pad the shorter documents with 0 for now. therefore now in Keras Embedding layer the 'input_length' will be equal to the length ie no of words in the document with maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of words in any document is :  9\n"
     ]
    }
   ],
   "source": [
    "# length of maximum document. will be nedded whenever create embeddings for the words\n",
    "maxi=-1\n",
    "for doc in corp:\n",
    "    tokens=nltk.word_tokenize(doc)\n",
    "    if(maxi<len(tokens)):\n",
    "        maxi=len(tokens)\n",
    "print(\"The maximum number of words in any document is : \",maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of padded documents:  3\n"
     ]
    }
   ],
   "source": [
    "# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\n",
    "pad_corp=pad_sequences(encod_corp,maxlen=maxi,padding='post',value=0.0)\n",
    "print(\"No of padded documents: \",len(pad_corp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The padded encoding for document 1  is :  [10 35 45 30  8 43 44 35 26]\n",
      "The padded encoding for document 2  is :  [26 17 44  3 26 19  0  0  0]\n",
      "The padded encoding for document 3  is :  [36 33 22 10  3  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "for i,doc in enumerate(pad_corp):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTUALLY CREATING THE EMBEDDINGS using KERAS EMBEDDING LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now all the documents are of same length. and so now we are ready to create and use the embeddings.\n",
    "\n",
    "** I will embed the words into vectors of 8 dimensions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=Input(shape=(3,9),dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input=Input(shape=(9,),dtype='float64')  # shape of input. each document has 9 element or words\n",
    "word_embedding=Embedding(input_dim=vocab_size,output_dim=8,input_length=maxi)(word_input) # creating the embedding\n",
    "word_vec=Flatten()(word_embedding) # flatten\n",
    "embed_model =Model([word_input],word_vec) # combining all into a Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc']) # compiling the model. parameters can be tuned as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"embedding_1/embedding_lookup:0\", shape=(?, 9, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(type(word_embedding))\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 72)                0         \n",
      "=================================================================\n",
      "Total params: 400\n",
      "Trainable params: 400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(embed_model.summary()) # summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=embed_model.predict(pad_corp) # finally getting the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings :  (3, 72)\n",
      "[[ 0.00130282 -0.00596644  0.02325323 -0.04432362 -0.04526334 -0.02512041\n",
      "  -0.02512375  0.01176981 -0.04361005  0.0228055  -0.01651942  0.04873116\n",
      "   0.03909346  0.02524883  0.01447764  0.0360704  -0.02517453  0.01186245\n",
      "   0.03737007 -0.02054998 -0.02924714 -0.01061837 -0.04460711  0.0448385\n",
      "   0.04469622  0.02031794  0.01015266  0.04472108 -0.04150879 -0.02770575\n",
      "  -0.0303477  -0.02566599 -0.00362151 -0.01438711 -0.0116431  -0.04811322\n",
      "  -0.01641407  0.0200397   0.04407834 -0.02589554 -0.01004182  0.00317496\n",
      "  -0.04504983  0.02963866 -0.02431839 -0.03926616 -0.03195803  0.0107885\n",
      "  -0.02775555 -0.00507268  0.04591706  0.01409023  0.04677561  0.04704653\n",
      "   0.02259154 -0.04589155 -0.04361005  0.0228055  -0.01651942  0.04873116\n",
      "   0.03909346  0.02524883  0.01447764  0.0360704  -0.04570699 -0.03431757\n",
      "   0.04339368 -0.02433269  0.0262607   0.03213033  0.03852773 -0.04842984]\n",
      " [-0.04570699 -0.03431757  0.04339368 -0.02433269  0.0262607   0.03213033\n",
      "   0.03852773 -0.04842984  0.01743628 -0.02529113 -0.04675657 -0.00031539\n",
      "   0.01406907  0.03230958 -0.04144602  0.01139281 -0.02775555 -0.00507268\n",
      "   0.04591706  0.01409023  0.04677561  0.04704653  0.02259154 -0.04589155\n",
      "   0.02951528 -0.04648987 -0.00377679  0.0289459  -0.03243428 -0.03955519\n",
      "  -0.01277244 -0.01818533 -0.04570699 -0.03431757  0.04339368 -0.02433269\n",
      "   0.0262607   0.03213033  0.03852773 -0.04842984 -0.02114774  0.04633969\n",
      "  -0.02501484 -0.02590113  0.00819069  0.02174618 -0.03460379 -0.03508151\n",
      "   0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074  0.02205712\n",
      "  -0.03504073  0.01487643  0.0315189  -0.00070102 -0.03515846 -0.02359458\n",
      "   0.01203074  0.02205712 -0.03504073  0.01487643  0.0315189  -0.00070102\n",
      "  -0.03515846 -0.02359458  0.01203074  0.02205712 -0.03504073  0.01487643]\n",
      " [-0.01663374 -0.03132546 -0.0126214   0.02948492  0.01633806  0.0308103\n",
      "   0.01902031 -0.04161846 -0.01743969 -0.01649902  0.00023746  0.00632057\n",
      "  -0.02120264 -0.03914863 -0.0456136  -0.00896112  0.0212315  -0.04152407\n",
      "  -0.00133731  0.03054306  0.04620203 -0.02571069  0.03679309 -0.02860994\n",
      "   0.00130282 -0.00596644  0.02325323 -0.04432362 -0.04526334 -0.02512041\n",
      "  -0.02512375  0.01176981  0.02951528 -0.04648987 -0.00377679  0.0289459\n",
      "  -0.03243428 -0.03955519 -0.01277244 -0.01818533  0.0315189  -0.00070102\n",
      "  -0.03515846 -0.02359458  0.01203074  0.02205712 -0.03504073  0.01487643\n",
      "   0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074  0.02205712\n",
      "  -0.03504073  0.01487643  0.0315189  -0.00070102 -0.03515846 -0.02359458\n",
      "   0.01203074  0.02205712 -0.03504073  0.01487643  0.0315189  -0.00070102\n",
      "  -0.03515846 -0.02359458  0.01203074  0.02205712 -0.03504073  0.01487643]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of embeddings : \",embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings :  (3, 9, 8)\n",
      "[[[ 0.00130282 -0.00596644  0.02325323 -0.04432362 -0.04526334\n",
      "   -0.02512041 -0.02512375  0.01176981]\n",
      "  [-0.04361005  0.0228055  -0.01651942  0.04873116  0.03909346\n",
      "    0.02524883  0.01447764  0.0360704 ]\n",
      "  [-0.02517453  0.01186245  0.03737007 -0.02054998 -0.02924714\n",
      "   -0.01061837 -0.04460711  0.0448385 ]\n",
      "  [ 0.04469622  0.02031794  0.01015266  0.04472108 -0.04150879\n",
      "   -0.02770575 -0.0303477  -0.02566599]\n",
      "  [-0.00362151 -0.01438711 -0.0116431  -0.04811322 -0.01641407\n",
      "    0.0200397   0.04407834 -0.02589554]\n",
      "  [-0.01004182  0.00317496 -0.04504983  0.02963866 -0.02431839\n",
      "   -0.03926616 -0.03195803  0.0107885 ]\n",
      "  [-0.02775555 -0.00507268  0.04591706  0.01409023  0.04677561\n",
      "    0.04704653  0.02259154 -0.04589155]\n",
      "  [-0.04361005  0.0228055  -0.01651942  0.04873116  0.03909346\n",
      "    0.02524883  0.01447764  0.0360704 ]\n",
      "  [-0.04570699 -0.03431757  0.04339368 -0.02433269  0.0262607\n",
      "    0.03213033  0.03852773 -0.04842984]]\n",
      "\n",
      " [[-0.04570699 -0.03431757  0.04339368 -0.02433269  0.0262607\n",
      "    0.03213033  0.03852773 -0.04842984]\n",
      "  [ 0.01743628 -0.02529113 -0.04675657 -0.00031539  0.01406907\n",
      "    0.03230958 -0.04144602  0.01139281]\n",
      "  [-0.02775555 -0.00507268  0.04591706  0.01409023  0.04677561\n",
      "    0.04704653  0.02259154 -0.04589155]\n",
      "  [ 0.02951528 -0.04648987 -0.00377679  0.0289459  -0.03243428\n",
      "   -0.03955519 -0.01277244 -0.01818533]\n",
      "  [-0.04570699 -0.03431757  0.04339368 -0.02433269  0.0262607\n",
      "    0.03213033  0.03852773 -0.04842984]\n",
      "  [-0.02114774  0.04633969 -0.02501484 -0.02590113  0.00819069\n",
      "    0.02174618 -0.03460379 -0.03508151]\n",
      "  [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074\n",
      "    0.02205712 -0.03504073  0.01487643]\n",
      "  [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074\n",
      "    0.02205712 -0.03504073  0.01487643]\n",
      "  [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074\n",
      "    0.02205712 -0.03504073  0.01487643]]\n",
      "\n",
      " [[-0.01663374 -0.03132546 -0.0126214   0.02948492  0.01633806\n",
      "    0.0308103   0.01902031 -0.04161846]\n",
      "  [-0.01743969 -0.01649902  0.00023746  0.00632057 -0.02120264\n",
      "   -0.03914863 -0.0456136  -0.00896112]\n",
      "  [ 0.0212315  -0.04152407 -0.00133731  0.03054306  0.04620203\n",
      "   -0.02571069  0.03679309 -0.02860994]\n",
      "  [ 0.00130282 -0.00596644  0.02325323 -0.04432362 -0.04526334\n",
      "   -0.02512041 -0.02512375  0.01176981]\n",
      "  [ 0.02951528 -0.04648987 -0.00377679  0.0289459  -0.03243428\n",
      "   -0.03955519 -0.01277244 -0.01818533]\n",
      "  [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074\n",
      "    0.02205712 -0.03504073  0.01487643]\n",
      "  [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074\n",
      "    0.02205712 -0.03504073  0.01487643]\n",
      "  [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074\n",
      "    0.02205712 -0.03504073  0.01487643]\n",
      "  [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074\n",
      "    0.02205712 -0.03504073  0.01487643]]]\n"
     ]
    }
   ],
   "source": [
    "embeddings=embeddings.reshape(-1,maxi,8)\n",
    "print(\"Shape of embeddings : \",embeddings.shape) \n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting shape is (3,9,8).\n",
    "\n",
    "**3---> no of documents**\n",
    "\n",
    "**9---> each document is made of 9 words which was our 'maxi' variable**\n",
    "\n",
    "**& 8---> each word is 8 dimensional.**\n",
    "\n",
    "To check this note that the encodings for '0' are same at the very end of output list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GETTING ENCODING FOR A PARTICULAR WORD IN A SPECIFIC DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding for  1 th word in 1 th document is : \n",
      "\n",
      " [ 0.00130282 -0.00596644  0.02325323 -0.04432362 -0.04526334 -0.02512041\n",
      " -0.02512375  0.01176981]\n",
      "The encoding for  2 th word in 1 th document is : \n",
      "\n",
      " [-0.04361005  0.0228055  -0.01651942  0.04873116  0.03909346  0.02524883\n",
      "  0.01447764  0.0360704 ]\n",
      "The encoding for  3 th word in 1 th document is : \n",
      "\n",
      " [-0.02517453  0.01186245  0.03737007 -0.02054998 -0.02924714 -0.01061837\n",
      " -0.04460711  0.0448385 ]\n",
      "The encoding for  4 th word in 1 th document is : \n",
      "\n",
      " [ 0.04469622  0.02031794  0.01015266  0.04472108 -0.04150879 -0.02770575\n",
      " -0.0303477  -0.02566599]\n",
      "The encoding for  5 th word in 1 th document is : \n",
      "\n",
      " [-0.00362151 -0.01438711 -0.0116431  -0.04811322 -0.01641407  0.0200397\n",
      "  0.04407834 -0.02589554]\n",
      "The encoding for  6 th word in 1 th document is : \n",
      "\n",
      " [-0.01004182  0.00317496 -0.04504983  0.02963866 -0.02431839 -0.03926616\n",
      " -0.03195803  0.0107885 ]\n",
      "The encoding for  7 th word in 1 th document is : \n",
      "\n",
      " [-0.02775555 -0.00507268  0.04591706  0.01409023  0.04677561  0.04704653\n",
      "  0.02259154 -0.04589155]\n",
      "The encoding for  8 th word in 1 th document is : \n",
      "\n",
      " [-0.04361005  0.0228055  -0.01651942  0.04873116  0.03909346  0.02524883\n",
      "  0.01447764  0.0360704 ]\n",
      "The encoding for  9 th word in 1 th document is : \n",
      "\n",
      " [-0.04570699 -0.03431757  0.04339368 -0.02433269  0.0262607   0.03213033\n",
      "  0.03852773 -0.04842984]\n",
      "The encoding for  1 th word in 2 th document is : \n",
      "\n",
      " [-0.04570699 -0.03431757  0.04339368 -0.02433269  0.0262607   0.03213033\n",
      "  0.03852773 -0.04842984]\n",
      "The encoding for  2 th word in 2 th document is : \n",
      "\n",
      " [ 0.01743628 -0.02529113 -0.04675657 -0.00031539  0.01406907  0.03230958\n",
      " -0.04144602  0.01139281]\n",
      "The encoding for  3 th word in 2 th document is : \n",
      "\n",
      " [-0.02775555 -0.00507268  0.04591706  0.01409023  0.04677561  0.04704653\n",
      "  0.02259154 -0.04589155]\n",
      "The encoding for  4 th word in 2 th document is : \n",
      "\n",
      " [ 0.02951528 -0.04648987 -0.00377679  0.0289459  -0.03243428 -0.03955519\n",
      " -0.01277244 -0.01818533]\n",
      "The encoding for  5 th word in 2 th document is : \n",
      "\n",
      " [-0.04570699 -0.03431757  0.04339368 -0.02433269  0.0262607   0.03213033\n",
      "  0.03852773 -0.04842984]\n",
      "The encoding for  6 th word in 2 th document is : \n",
      "\n",
      " [-0.02114774  0.04633969 -0.02501484 -0.02590113  0.00819069  0.02174618\n",
      " -0.03460379 -0.03508151]\n",
      "The encoding for  7 th word in 2 th document is : \n",
      "\n",
      " [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074  0.02205712\n",
      " -0.03504073  0.01487643]\n",
      "The encoding for  8 th word in 2 th document is : \n",
      "\n",
      " [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074  0.02205712\n",
      " -0.03504073  0.01487643]\n",
      "The encoding for  9 th word in 2 th document is : \n",
      "\n",
      " [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074  0.02205712\n",
      " -0.03504073  0.01487643]\n",
      "The encoding for  1 th word in 3 th document is : \n",
      "\n",
      " [-0.01663374 -0.03132546 -0.0126214   0.02948492  0.01633806  0.0308103\n",
      "  0.01902031 -0.04161846]\n",
      "The encoding for  2 th word in 3 th document is : \n",
      "\n",
      " [-0.01743969 -0.01649902  0.00023746  0.00632057 -0.02120264 -0.03914863\n",
      " -0.0456136  -0.00896112]\n",
      "The encoding for  3 th word in 3 th document is : \n",
      "\n",
      " [ 0.0212315  -0.04152407 -0.00133731  0.03054306  0.04620203 -0.02571069\n",
      "  0.03679309 -0.02860994]\n",
      "The encoding for  4 th word in 3 th document is : \n",
      "\n",
      " [ 0.00130282 -0.00596644  0.02325323 -0.04432362 -0.04526334 -0.02512041\n",
      " -0.02512375  0.01176981]\n",
      "The encoding for  5 th word in 3 th document is : \n",
      "\n",
      " [ 0.02951528 -0.04648987 -0.00377679  0.0289459  -0.03243428 -0.03955519\n",
      " -0.01277244 -0.01818533]\n",
      "The encoding for  6 th word in 3 th document is : \n",
      "\n",
      " [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074  0.02205712\n",
      " -0.03504073  0.01487643]\n",
      "The encoding for  7 th word in 3 th document is : \n",
      "\n",
      " [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074  0.02205712\n",
      " -0.03504073  0.01487643]\n",
      "The encoding for  8 th word in 3 th document is : \n",
      "\n",
      " [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074  0.02205712\n",
      " -0.03504073  0.01487643]\n",
      "The encoding for  9 th word in 3 th document is : \n",
      "\n",
      " [ 0.0315189  -0.00070102 -0.03515846 -0.02359458  0.01203074  0.02205712\n",
      " -0.03504073  0.01487643]\n"
     ]
    }
   ],
   "source": [
    "for i,doc in enumerate(embeddings):\n",
    "    for j,word in enumerate(doc):\n",
    "        print(\"The encoding for \",j+1,\"th word\",\"in\",i+1,\"th document is : \\n\\n\",word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now this makes it easier to visualize that we have 3(size of corp) documents with each consisting of 9(maxi) words and each word mapped to a 8-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
